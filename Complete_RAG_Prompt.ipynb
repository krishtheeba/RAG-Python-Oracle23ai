{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319bd5bf-06b2-4881-912e-2c7093359b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61bc4e-4378-463f-89f4-347b0748ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain(\"what is langchain and how to write factorial program?\")\n",
    "|\n",
    "Your RAG is \"hallucinating\" when the query mixes two unrelated topics.\n",
    "|\n",
    "contains two completely different questions:\n",
    "\n",
    "What is LangChain? → present in your documents\n",
    "\n",
    "How to write factorial program? → NOT in your documents\n",
    "\n",
    "Your RetrievalQA chain does this:\n",
    "\n",
    "Looks for relevant documents\n",
    "\n",
    "Finds only LangChain-related chunks\n",
    "\n",
    "Sends your FULL question + LangChain context to the LLM\n",
    "\n",
    "LLM tries to answer both parts\n",
    "\n",
    "Because context is only about LangChain, it fuses them → hallucination\n",
    "\n",
    "That's why it produced this nonsense\n",
    "To write a factorial program using LangChain...\n",
    "This is a classic RAG hallucination.\n",
    "|\n",
    "✔️ Why this happens (important to understand)\n",
    "\n",
    "Retriever result for your long question:\n",
    "\n",
    "Relevant chunks = only LangChain text\n",
    "Missing = factorial program chunk\n",
    "\n",
    "But the prompt to LLM still contains:\n",
    "======================================\n",
    "Context:\n",
    "<LangChain text>\n",
    "Question:\n",
    "\"what is langchain and how to write factorial program?\"\n",
    "\n",
    "So LLM thinks:\n",
    "\n",
    "“The question asks two things. But the only context I see is LangChain. So maybe factorial program is also related to LangChain?”\n",
    "\n",
    "Therefore, hallucination.\n",
    "|\n",
    "|\n",
    " Solution: Prevent LLM from answering questions NOT supported by context\n",
    "\n",
    "You need a strict RAG safety prompt:\n",
    "\n",
    "Strict RAG Prompt\n",
    "|\n",
    "template = \"\"\"\n",
    "You are a Retrieval based QA assistant.\n",
    "\n",
    "Answer ONLY from the given context.\n",
    "If the answer is not present in the context, say:\n",
    "\"I don't know, the document does not contain this information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "Now LLM will NOT hallucinate.\n",
    "|\n",
    "|\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a Retrieval QA assistant.\n",
    "\n",
    "Answer ONLY using the information in the context.\n",
    "If the answer is not present in the context, reply:\n",
    "\"I don't know, the document does not contain this information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_obj,\n",
    "    retriever=retriever_obj,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "Now try:\n",
    "qa_chain(\"what is langchain and how to write factorial program?\")\n",
    " Expected output:\n",
    "I don't know, the document does not contain this information.\n",
    "And NOT hallucination.\n",
    "\n",
    "If you want hybrid behavior (RAG + fallback to model), use this:\n",
    "If retrieval fails, LLM uses its own knowledge.\n",
    "Hybrid Prompt\n",
    "template = \"\"\"\n",
    "Use the context to answer the question.\n",
    "If the context does not contain the answer, answer from your general knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    " Summary\n",
    "| Prompt Type    | Behavior                                         |\n",
    "| -------------- | ------------------------------------------------ |\n",
    "| Strict RAG     | Avoid hallucination. Only answer from docs.      |\n",
    "| Hybrid RAG     | Answer from docs + fallback to LLM knowledge.    |\n",
    "| Default Prompt | High chance of hallucination in mixed questions. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d181cfdd-25e0-4039-8fa4-9b7d34585b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8006c20f-865b-4885-8141-b996cd277253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 250\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_15328\\2726649567.py:64: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(qa_chain(\"What is LangChain?\"))\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is LangChain?', 'result': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'source_documents': [Document(id='e567c9c0-8a0e-4e80-8867-12b5ce3ade2b', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='19de1199-a38c-4a8f-86a9-21ca17aa2878', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='b4088c1c-743a-4525-8226-f4c9c772e400', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write factorial program in C?', 'result': \"I don't know, the document does not contain this information.\", 'source_documents': [Document(id='b4088c1c-743a-4525-8226-f4c9c772e400', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120'), Document(id='19de1199-a38c-4a8f-86a9-21ca17aa2878', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='e567c9c0-8a0e-4e80-8867-12b5ce3ade2b', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:')]}\n",
      "{'query': 'What is LangChain and how to write factorial program?', 'result': \"I don't know, the document does not contain this information.\", 'source_documents': [Document(id='e567c9c0-8a0e-4e80-8867-12b5ce3ade2b', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='19de1199-a38c-4a8f-86a9-21ca17aa2878', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='b4088c1c-743a-4525-8226-f4c9c772e400', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Retrive from loaded documents only not from trained LLM\n",
    "# ------------------------------------------------------------\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Step-1 Load your text file\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step-2 Split into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Step-3 Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step-4 Vector DB\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step-5 Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step-6 Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# Step-7 STRICT RAG Prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a strict Retrieval QA assistant.\n",
    "Answer ONLY using the information given in the context below.\n",
    "If the answer is not present in the context, reply exactly with:\n",
    "\"I don't know, the document does not contain this information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step-8 Build the RAG Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Try Questions\n",
    "print(qa_chain(\"What is LangChain?\"))\n",
    "print(qa_chain(\"How to write factorial program in C?\"))\n",
    "print(qa_chain(\"What is LangChain and how to write factorial program?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3316bbd7-89b4-46bc-89a7-300da174fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 467, which is longer than the specified 200\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is langchain?', 'result': 'Based on the given context, LangChain is a framework for developing applications powered by large language models (LLMs).', 'source_documents': [Document(id='1066df08-024c-4d54-880a-905510d320b8', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='2a2b5331-94aa-4820-a847-3fe56c35b050', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='619248be-d24a-45dc-b1dc-1258af201157', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to write factorial program in C language?', 'result': 'The given context does not provide any information about writing a factorial program in C language. However, I can provide a general answer based on my knowledge.\\n\\nTo write a factorial program in C language, you can use the following code:\\n\\n```c\\n#include <stdio.h>\\n\\nint factorial(int n) {\\n    if (n == 0) {\\n        return 1;\\n    } else {\\n        return n * factorial(n - 1);\\n    }\\n}\\n\\nint main() {\\n    int num;\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n\\n    if (num < 0) {\\n        printf(\"Factorial is not defined for negative numbers.\");\\n    } else {\\n        int result = factorial(num);\\n        printf(\"Factorial of %d is %d\\\\n\", num, result);\\n    }\\n\\n    return 0;\\n}\\n```\\n\\nThis program calculates the factorial of a given number using recursion. The `factorial` function calls itself with decreasing values until it reaches the base case (n = 0), and then it starts returning the products of the numbers.\\n\\nYou can compile and run this code to see the factorial of a number.', 'source_documents': [Document(id='619248be-d24a-45dc-b1dc-1258af201157', metadata={'source': 'my_docs.txt'}, page_content='factorial value 5! is 120'), Document(id='2a2b5331-94aa-4820-a847-3fe56c35b050', metadata={'source': 'my_docs.txt'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='1066df08-024c-4d54-880a-905510d320b8', metadata={'source': 'my_docs.txt'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:')]}\n"
     ]
    }
   ],
   "source": [
    "# Hybrid RAG Answer from docs + fallback to LLM knowledge\n",
    "#\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Step-1 load data\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step-2 split\n",
    "splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# Step-3 embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Step-4 vector DB\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step-5 retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step-6 Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# Step-7 Better Prompt → fallback enabled\n",
    "template = \"\"\"\n",
    "Use the following context to answer the question.\n",
    "If the answer is not present in the context, answer from your general knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Step-8 Questions\n",
    "print(qa_chain(\"what is langchain?\"))\n",
    "print(qa_chain(\"How to write factorial program in C language?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3433e2-38dd-4108-bda4-39aac236fb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
