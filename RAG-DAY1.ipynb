{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47a5aea-f0a9-4e7c-85f9-8762a63efcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea1f20b-a270-4ca2-b7f0-04d637d2f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" i like to read machnine learning algorithm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445388d-3262-4459-a7b5-e281b7ef224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookUp Error\n",
    ">>> import nltk\n",
    ">>> nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a7f87-9535-4b94-8808-99fdb0512dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "documents\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b21f41-2e30-4438-987f-3d588a4ce2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'to', 'read', 'machnine', 'learning', 'algorithm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a7651b-a344-4122-ac2c-9d50e22aa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=''' hello  welcome to Rag training .\n",
    "lets learn NLP , then LLM, Chaining\n",
    "Oracle 23ai, prompting '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5324a2-a417-4e91-948c-c4fdb7a5df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a457137a-1012-4b62-972f-cbab53175174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' hello  welcome to Rag training .', 'lets learn NLP , then LLM, Chaining\\nOracle 23ai, prompting']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b344f6-1f79-4713-9e88-a80fb247c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=\"Hell0 Arun, how are you?. ok. Iam doing well. hos is activities's code \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0b5f64-5c1a-475e-8c3e-9706316104f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hell0',\n",
       " 'Arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '.',\n",
       " 'ok.',\n",
       " 'Iam',\n",
       " 'doing',\n",
       " 'well',\n",
       " '.',\n",
       " 'hos',\n",
       " 'is',\n",
       " 'activities',\n",
       " \"'s\",\n",
       " 'code']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb14ae8f-75b5-44c3-856f-87740d8dc92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hell0',\n",
       " 'Arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?.',\n",
       " 'ok',\n",
       " '.',\n",
       " 'Iam',\n",
       " 'doing',\n",
       " 'well',\n",
       " '.',\n",
       " 'hos',\n",
       " 'is',\n",
       " 'activities',\n",
       " \"'\",\n",
       " 's',\n",
       " 'code']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(msg)|-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767fd93-f70c-45d0-a7eb-544d38a847af",
   "metadata": {},
   "outputs": [],
   "source": [
    "TreebankTokenizer\n",
    "|\n",
    "class\n",
    " |- handles apostrophies\n",
    "|- split punctuation\n",
    "|- adds space \n",
    "\n",
    "can't --> \"ca\"  + \"n't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32e9111d-f03c-4356-96eb-2105414673ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method RegexpTokenizer.tokenize of WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df39569e-3168-4cf3-8b2c-f50f56f15eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.treebank.TreebankWordTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "print(TreebankWordTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526fd36-4b74-449a-a9f5-dfba9fc781f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk\n",
    "|----tokenize\n",
    "       |-------treebank.py\n",
    "                    |\n",
    "                    class TreeBankTokenizer:\n",
    "                                    def tokenize(self. ):\n",
    "                                         .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10fc25e9-4d52-409f-b3c7-9e6e1eeb7695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hell0 Arun, how are you?. ok. Iam doing well. hos is activities's code \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49020eb9-51f2-4958-8f44-d3c5f90e5ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hell0',\n",
       " 'Arun',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '.',\n",
       " 'ok.',\n",
       " 'Iam',\n",
       " 'doing',\n",
       " 'well.',\n",
       " 'how',\n",
       " 'is',\n",
       " 'activities',\n",
       " \"'s\",\n",
       " 'code',\n",
       " 'ab',\n",
       " \"'s\",\n",
       " 'data1.c']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg=\"Hell0 Arun, how are you?. ok. Iam doing well. how is activities's code  ab's data1.c\"\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "obj=TreebankWordTokenizer()\n",
    "obj.tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb4b482-421a-4581-b353-696900bb3837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp term\n",
    "# Stemming - reduce to the root words\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "obj=PorterStemmer()\n",
    "obj.stem(\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3363deaa-d521-4404-998b-e497ae40c65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dde1340-327d-4f6d-9b98-093700d7f046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c0acfe1-4324-4fec-9201-c3d237a00fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stem = RegexpStemmer('ing$|s$|es$') # user specific regx pattern\n",
    "reg_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5a5fd10-31f0-4655-a019-01143f4b8107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbfa44-f631-45d2-81ea-12bae35a4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization - Lemma\n",
    "# reducing to root word based pos = v,av,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fdd1616-87ad-43ce-93a5-fd050a053cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma_obj=WordNetLemmatizer()\n",
    "lemma_obj.lemmatize(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a36dda0-014b-416c-bf94-a2b659a384f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_obj.lemmatize(\"eating\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e078d2-544c-4697-b7a6-c24e5a51b7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_obj.lemmatize(\"history\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c170f-3d96-4e4b-82b0-779f5dacab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords - NLP reprocessing\n",
    " |------> do not carry any meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "341113fd-84a5-4d1a-8270-b47b8549dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9a6fc5-2718-4853-9a64-9c07c2f4ee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')\n",
    "#stopwords.words('arabic')\n",
    "#stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8528b-30bc-428c-a96c-50e5242b2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization---> token - reduce this token to root word- stem-> lemma -> stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b89b067-ad32-41cc-b696-c798ce579213",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gn_list=[\"words\",\"eats\",\"eatern\",\"writing\",\n",
    "         \"writes\",\"programming\",\"program\",\n",
    "         \"history\",\"finally\",\"finalized\"]\n",
    "# import nltk module\n",
    "  #         |-- porterStemmer  - root words\n",
    "  #      |--- WordNetLemmatizer - pos = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e698d3c-be11-4d66-b9af-7e961816f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words ----->  word\n",
      "eats ----->  eat\n",
      "eatern ----->  eatern\n",
      "writing ----->  write\n",
      "writes ----->  write\n",
      "programming ----->  program\n",
      "program ----->  program\n",
      "history ----->  histori\n",
      "finally ----->  final\n",
      "finalized ----->  final\n"
     ]
    }
   ],
   "source": [
    "Gn_list=[\"words\",\"eats\",\"eatern\",\"writing\",\n",
    "         \"writes\",\"programming\",\"program\",\n",
    "         \"history\",\"finally\",\"finalized\"]\n",
    "obj=PorterStemmer()\n",
    "for var in Gn_list:\n",
    "    print(f'{var} ----->  {obj.stem(var)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8823186c-e7d0-4879-960d-0839e2ac798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words----> word\n",
      "eats----> eat\n",
      "eatern----> eatern\n",
      "writing----> write\n",
      "writes----> write\n",
      "programming----> program\n",
      "program----> program\n",
      "history----> history\n",
      "finally----> finally\n",
      "finalized----> finalize\n"
     ]
    }
   ],
   "source": [
    "lemma_obj = WordNetLemmatizer()\n",
    "for var in Gn_list:\n",
    "    print(f\"{var}----> { lemma_obj.lemmatize(var,pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30247224-a6ee-4108-a74e-d9e39c8fc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example displaying of list or collection of stop words to filter\n"
     ]
    }
   ],
   "source": [
    "msg=\"This is an example displaying of list or collection of stop words to filter\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "144d114e-6349-4165-823f-119ab0556cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'displaying',\n",
       " 'of',\n",
       " 'list',\n",
       " 'or',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'to',\n",
       " 'filter']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39dab93d-bdb6-4a58-9a29-7622551f04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "displaying\n",
      "list\n",
      "collection\n",
      "stop\n",
      "words\n",
      "filter\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(msg)\n",
    "for var in words:\n",
    "    if var.lower() not in stopwords.words('english'):\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55f1564f-63c4-43a9-8f80-f3fd34a83d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'displaying', 'list', 'collection', 'stop', 'words', 'filter']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_words= [var for var in words if var.lower() not in stopwords.words('english')]  # Functional Style- List comprehension\n",
    "filter_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8081f48-dfa0-42e1-aca9-23ceeca9dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task\n",
    "#====\n",
    "Corpus\n",
    "|--- Use PorterStemmer  and WordNetLemmatizer()\n",
    "|--- Apply StopWords\n",
    "|--- Filter remaining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb9b58c2-da88-4b55-86a9-8de0abdd1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Vikram Sarabhai was born on 12 August 1919 in a Gujarati Śvetāmbara Shrimali Jain family, \n",
    "in Ahmedabad, India.His father was Ambalal Sarabhai,\n",
    "a major industrialist committed to the Indian independence movement.\n",
    "Known as the cradle of space sciences in India, the Physical Research Laboratory (PRL) was\n",
    "founded in 1947 by Vikram Sarabhai. PRL had a modest beginning at his residence, the \"RETREAT\", \n",
    "with research on cosmic rays.\n",
    "The institute was formally established at the M.G. Science Institute, Ahmedabad, on 11 November 1947 with support \n",
    "from the Karmkshetra Educational Foundation and the Ahmedabad Education Society. Kalpathi Ramakrishna Ramanathan \n",
    "was the first director of the institute. The initial focus was research on cosmic rays and the properties of the\n",
    "upper atmosphere. Research areas were expanded to include theoretical physics and radio physics later\n",
    "with grants from the Atomic Energy Commission. \n",
    "He led the Sarabhai family-owned business conglomerate.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6904133-be8b-4b74-8410-6feac05ea14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 9\n",
      "Vikram Sarabhai was born on 12 August 1919 in a Gujarati Śvetāmbara Shrimali Jain family, \n",
      "in Ahmedabad, India.His father was Ambalal Sarabhai,\n",
      "a major industrialist committed to the Indian independence movement.\n",
      "Known as the cradle of space sciences in India, the Physical Research Laboratory (PRL) was\n",
      "founded in 1947 by Vikram Sarabhai.\n",
      "PRL had a modest beginning at his residence, the \"RETREAT\", \n",
      "with research on cosmic rays.\n",
      "The institute was formally established at the M.G.\n",
      "Science Institute, Ahmedabad, on 11 November 1947 with support \n",
      "from the Karmkshetra Educational Foundation and the Ahmedabad Education Society.\n",
      "Kalpathi Ramakrishna Ramanathan \n",
      "was the first director of the institute.\n",
      "The initial focus was research on cosmic rays and the properties of the\n",
      "upper atmosphere.\n",
      "Research areas were expanded to include theoretical physics and radio physics later\n",
      "with grants from the Atomic Energy Commission.\n",
      "He led the Sarabhai family-owned business conglomerate.\n"
     ]
    }
   ],
   "source": [
    "docs= sent_tokenize(corpus)\n",
    "print(type(docs), len(docs))\n",
    "for var in docs:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f7cdf55-bbfa-420b-8d9a-9378e99175b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vikram', 'Sarabhai', 'was', 'born', 'on', '12', 'August', '1919', 'in', 'a', 'Gujarati', 'Śvetāmbara', 'Shrimali', 'Jain', 'family', ',', 'in', 'Ahmedabad', ',', 'India.His', 'father', 'was', 'Ambalal', 'Sarabhai', ',', 'a', 'major', 'industrialist', 'committed', 'to', 'the', 'Indian', 'independence', 'movement', '.']\n",
      "['Known', 'as', 'the', 'cradle', 'of', 'space', 'sciences', 'in', 'India', ',', 'the', 'Physical', 'Research', 'Laboratory', '(', 'PRL', ')', 'was', 'founded', 'in', '1947', 'by', 'Vikram', 'Sarabhai', '.']\n",
      "['PRL', 'had', 'a', 'modest', 'beginning', 'at', 'his', 'residence', ',', 'the', '``', 'RETREAT', \"''\", ',', 'with', 'research', 'on', 'cosmic', 'rays', '.']\n",
      "['The', 'institute', 'was', 'formally', 'established', 'at', 'the', 'M.G', '.']\n",
      "['Science', 'Institute', ',', 'Ahmedabad', ',', 'on', '11', 'November', '1947', 'with', 'support', 'from', 'the', 'Karmkshetra', 'Educational', 'Foundation', 'and', 'the', 'Ahmedabad', 'Education', 'Society', '.']\n",
      "['Kalpathi', 'Ramakrishna', 'Ramanathan', 'was', 'the', 'first', 'director', 'of', 'the', 'institute', '.']\n",
      "['The', 'initial', 'focus', 'was', 'research', 'on', 'cosmic', 'rays', 'and', 'the', 'properties', 'of', 'the', 'upper', 'atmosphere', '.']\n",
      "['Research', 'areas', 'were', 'expanded', 'to', 'include', 'theoretical', 'physics', 'and', 'radio', 'physics', 'later', 'with', 'grants', 'from', 'the', 'Atomic', 'Energy', 'Commission', '.']\n",
      "['He', 'led', 'the', 'Sarabhai', 'family-owned', 'business', 'conglomerate', '.']\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(docs)):\n",
    "    words= word_tokenize(docs[var])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecbc7b6d-cf61-421c-a0a7-94f1242e9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vikram\n",
      "sarabhai\n",
      "born\n",
      "12\n",
      "august\n",
      "1919\n",
      "gujarati\n",
      "śvetāmbara\n",
      "shrimali\n",
      "jain\n",
      "family\n",
      ",\n",
      "ahmedabad\n",
      ",\n",
      "india.his\n",
      "father\n",
      "ambalal\n",
      "sarabhai\n",
      ",\n",
      "major\n",
      "industrialist\n",
      "committed\n",
      "indian\n",
      "independence\n",
      "movement\n",
      ".\n",
      "known\n",
      "cradle\n",
      "space\n",
      "sciences\n",
      "india\n",
      ",\n",
      "physical\n",
      "research\n",
      "laboratory\n",
      "(\n",
      "prl\n",
      ")\n",
      "founded\n",
      "1947\n",
      "vikram\n",
      "sarabhai\n",
      ".\n",
      "prl\n",
      "modest\n",
      "beginning\n",
      "residence\n",
      ",\n",
      "``\n",
      "retreat\n",
      "''\n",
      ",\n",
      "research\n",
      "cosmic\n",
      "rays\n",
      ".\n",
      "institute\n",
      "formally\n",
      "established\n",
      "m.g\n",
      ".\n",
      "science\n",
      "institute\n",
      ",\n",
      "ahmedabad\n",
      ",\n",
      "11\n",
      "november\n",
      "1947\n",
      "support\n",
      "karmkshetra\n",
      "educational\n",
      "foundation\n",
      "ahmedabad\n",
      "education\n",
      "society\n",
      ".\n",
      "kalpathi\n",
      "ramakrishna\n",
      "ramanathan\n",
      "first\n",
      "director\n",
      "institute\n",
      ".\n",
      "initial\n",
      "focus\n",
      "research\n",
      "cosmic\n",
      "rays\n",
      "properties\n",
      "upper\n",
      "atmosphere\n",
      ".\n",
      "research\n",
      "areas\n",
      "expanded\n",
      "include\n",
      "theoretical\n",
      "physics\n",
      "radio\n",
      "physics\n",
      "later\n",
      "grants\n",
      "atomic\n",
      "energy\n",
      "commission\n",
      ".\n",
      "led\n",
      "sarabhai\n",
      "family-owned\n",
      "business\n",
      "conglomerate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for var in range(len(docs)):\n",
    "    words= word_tokenize(docs[var])\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a964e51-c226-4b19-9597-5732912aeb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data1', 'data2'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=['data1','data2','data1','data1'] \n",
    "set(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "073d8d88-8832-49bc-82db-2b09d384bf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data1', 'data2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "980d517c-3dc5-469e-9fac-3a1543af5318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vikram sarabhai born 12 august 1919 gujarati śvetāmbara shrimali jain famili , ahmedabad , india.hi father ambal sarabhai , major industrialist commit indian independ movement .',\n",
       " 'known cradl space scienc india , physic research laboratori ( prl ) found 1947 vikram sarabhai .',\n",
       " \"prl modest begin resid , `` retreat '' , research cosmic ray .\",\n",
       " 'the institut formal establish m.g .',\n",
       " 'scienc institut , ahmedabad , 11 novemb 1947 support karmkshetra educ foundat ahmedabad educ societi .',\n",
       " 'kalpathi ramakrishna ramanathan first director institut .',\n",
       " 'the initi focu research cosmic ray properti upper atmospher .',\n",
       " 'research area expand includ theoret physic radio physic later grant atom energi commiss .',\n",
       " 'he led sarabhai family-own busi conglomer .']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stem_obj=PorterStemmer()\n",
    "docs= nltk.sent_tokenize(corpus)\n",
    "\n",
    "for var in range(len(docs)):\n",
    "    words=nltk.word_tokenize(docs[var])\n",
    "    words= [stem_obj.stem(word)for word in words if word not in stopwords.words('english')]\n",
    "    docs[var] =\" \". join(words)\n",
    "\n",
    "docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ece84480-22e8-470e-a2a3-b8130439468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[', bear major 12 . Sarabhai father Ambalal Shrimali Gujarati August Jain movement Indian Śvetāmbara independence commit family industrialist Ahmedabad India.His 1919 Vikram',\n",
       " 'Sarabhai sciences space Research found Known 1947 Laboratory . Physical Vikram , ( ) PRL cradle India',\n",
       " \"RETREAT `` research begin , '' residence cosmic ray modest PRL .\",\n",
       " 'The institute formally establish M.G .',\n",
       " 'Educational 1947 support . Ahmedabad November Science Foundation Institute 11 , Society Karmkshetra Education',\n",
       " 'Kalpathi institute director Ramakrishna Ramanathan first .',\n",
       " 'research atmosphere upper focus The initial ray properties cosmic .',\n",
       " 'Research physics theoretical include Energy areas radio later Commission grant Atomic . expand',\n",
       " 'Sarabhai conglomerate business lead He family-owned .']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use LEmmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "docs= nltk.sent_tokenize(corpus)\n",
    "\n",
    "for var in range(len(docs)):\n",
    "    words=nltk.word_tokenize(docs[var])\n",
    "    words= [lemmatizer.lemmatize(word,pos=\"v\")for word in words if word not in stopwords.words('english')]\n",
    "    l=set(words)\n",
    "    words=list(l)\n",
    "    docs[var] =\" \". join(words)\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1443c19-715e-4abb-8d71-34bbc459d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram technique\n",
    "===============\n",
    "    An n-gram - continuous sequence of n items from a corpus\n",
    "\n",
    "food good bad => after lemmatization and filting stopwords\n",
    "n=3 => no. of unique\n",
    "\n",
    "n=1\n",
    "food --> 1 0 0 --> [1 0 0 ]\n",
    "good --> 0 1 1  -> [0 1 0]\n",
    "bad --> 0 0 1  ->  [ 0 0 1]\n",
    "\n",
    "\n",
    "n=2\n",
    "---\n",
    "food good   ->  1 0\n",
    "----------   \n",
    "good bad    ---> 0 1\n",
    "\n",
    "n=3\n",
    "food good bad  -> 1\n",
    "                  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dfc708b-cdb3-466c-90ad-4926290710c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ec36c1c-b9d7-4eab-bdd3-5b23d6513504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['likes', 'read', 'nlp', 'applications']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=\"likes read nlp applications\"\n",
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f29222dd-080b-42fd-a8ac-48a7aebb344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000001EE457E9A40>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token= word_tokenize(data)\n",
    "ngrams(token,1)  # unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf3297-64ae-40af-b154-cdb53b3ac82f",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def f1():\n",
    "...     return 10\n",
    "...\n",
    ">>> f1()\n",
    "10\n",
    ">>> def f2():\n",
    "...     yield 10\n",
    "...\n",
    ">>> f2()\n",
    "<generator object f2 at 0x0000014514D6C5F0>\n",
    ">>> # function returns an iterator obj--> generator\n",
    ">>>\n",
    ">>> next(f2())\n",
    "10\n",
    ">>>\n",
    ">>> for var in f2():\n",
    "...     print(var)\n",
    "...\n",
    "10\n",
    ">>> list(f2())\n",
    "[10]\n",
    ">>> def f3():\n",
    "...     yield 10\n",
    "...     yield [1,2,3]\n",
    "...     yield 1,\n",
    "...\n",
    ">>> f3()\n",
    "<generator object f3 at 0x0000014514D6DE00>\n",
    ">>> next(f3())\n",
    "10\n",
    ">>> next(f3())\n",
    "10\n",
    ">>> next(f3())\n",
    "10\n",
    ">>> for var in f3():\n",
    "...     print(var)\n",
    "...\n",
    "10\n",
    "[1, 2, 3]\n",
    "(1,)\n",
    ">>> list(f3())\n",
    "[10, [1, 2, 3], (1,)]\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe49a40c-14e1-4d95-b230-3c1ee5c0a3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes',), ('read',), ('nlp',), ('applications',)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(token,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef540c20-1e3f-4ec5-8168-72632262fcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read'), ('read', 'nlp'), ('nlp', 'applications')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(token,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77f3e954-56ae-4ed2-8bf2-e90c8ddae492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp'), ('read', 'nlp', 'applications')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(token,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b362b6d-f653-49db-a3d0-557f8c56e748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likes', 'read', 'nlp', 'applications')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(token,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92b4f37b-c3a3-44c2-99ef-12a1bdd8dc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(token,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab0f0837-6fe8-4d8b-9a24-0523c6fd103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['bad', 'food', 'good'], dtype='<U4')]\n",
      "\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "data=['food', 'good', 'bad']\n",
    "\n",
    "data= np.array(data).reshape(-1,1) # 2D\n",
    "\n",
    "encoder_obj=OneHotEncoder(sparse_output=False)\n",
    "one_hot=encoder_obj.fit_transform(data)\n",
    "print(encoder_obj.categories_)\n",
    "print('')\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c67d611-e1ee-48fa-a7cf-e55a168a6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['father', 'he', 'indian', 'program', 'regards', 'space', 'widely'],\n",
      "      dtype='<U7')]\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data=[\"he\",\"widely\",\"regards\",\"father\", \"indian\",\"space\",\"program\"]\n",
    "data= np.array(data).reshape(-1,1) # 2D\n",
    "\n",
    "encoder_obj=OneHotEncoder(sparse_output=False)\n",
    "one_hot=encoder_obj.fit_transform(data)\n",
    "print(encoder_obj.categories_)\n",
    "print('')\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c1805-ac7e-48ad-9575-817de976c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############  BREAK 1:05 PM to 2:05PM IST  #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470b7d6-8e80-4076-8d9f-db5133fb8d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f944c-7d60-4e64-9ea0-4a15b2f0e12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d5463-0878-48f3-b472-85c488584855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349d9b7-d72c-4596-885f-5118dbcacaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee24aca-a74b-49cc-ad99-75bca5a8fbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33effec-8ebb-4eca-93cf-de2df10c745a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d541580-3fbf-49cd-bc74-1431c9edff2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28362a40-48b4-49b4-aa0e-374dbb54cc30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Spark)",
   "language": "python",
   "name": "spark310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
